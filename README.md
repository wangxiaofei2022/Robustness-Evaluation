# Benchmarking Neural Network Robustness to Common Noises and Natural Scenes

This repository contains datasets and code for generating typical noisy and natural scenes from the paper "CBA: Contextual Background Attack against Optical Aerial Detection in the Physical World".

Requires Python 3.7 and PyTorch 1.8.0. 

# Robustness test pictures

![GitHub Logo](/example1.png)

The robustness test pictures contain four typical noises and three natural scenes, and each noise scene can generate five kinds of noise-added pictures with different intensities. The noise generated by this method, especially the snow scene, is closer to the actual natural scene.

# Benchmarking Neural Network Robustness to  Adversarial Attacks

In this benchmark, we evaluate adversarial robustness with 5 digital attacks, including Fast Gradient Sign Method (FGSM), AutoAttack (AA), Projected Gradient Descent (PGD), C&W, Momentum Iterative FGSM (MIFGSM).  Furthermore, we conduct the aforementioned attacks in both white-box and black-box conditions.

# Citation

If you find this useful in your research, please consider citing:

    @article{lian2023cba,
    author={Lian, Jiawei and Wang, Xiaofei and Su, Yuru and Ma, Mingyang and Mei, Shaohui},
    journal={IEEE Transactions on Geoscience and Remote Sensing}, 
    title={CBA: Contextual Background Attack Against Optical Aerial Detection in the Physical World}, 
    year={2023},
    volume={61},
    pages={1--16},
    publisher={IEEE}
    }
    @article{lian2022benchmarking,
    title={Benchmarking Adversarial Patch Against Aerial Detection},
    author={Lian, Jiawei and Mei, Shaohui and Zhang, Shun and Ma, Mingyang},
    journal={IEEE Transactions on Geoscience and Remote Sensing},
    volume={60},
    pages={1--16},
    year={2022},
    publisher={IEEE}
    }
